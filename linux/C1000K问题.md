首先从物理资源使用上来说，100 万个请求需要大量的系统资源。比如，

- 假设每个请求需要 16KB 内存的话，那么总共就需要大约 15 GB 内存。
- 而从带宽上来说，假设只有 20% 活跃连接，即使每个连接只需要 1KB/s 的吞吐量，总共也需要 1.6 Gb/s 的吞吐量。千兆网卡显然满足不了这么大的吞吐量，所以还需要配置万兆网卡，或者基于多网卡 Bonding 承载更大的吞吐量。

其次，从软件资源上来说，大量的连接也会占用大量的软件资源，比如文件描述符的数量、连接状态的跟踪（CONNTRACK）、网络协议栈的缓存大小（比如套接字读写缓存、TCP 读写缓存）等等。

最后，大量请求带来的中断处理，也会带来非常高的处理成本。这样，就需要多队列网卡、中断负载均衡、CPU 绑定、RPS/RFS（软中断负载均衡到多个 CPU 核上），以及将网络包的处理卸载（Offload）到网络设备（如 TSO/GSO、LRO/GRO、VXLAN OFFLOAD）等各种硬件和软件的优化。

C1000K 的解决方法，本质上还是构建在 epoll 的非阻塞 I/O 模型上。只不过，除了 I/O 模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能。



### 100w连接需要的资源分配和参数调整

**系统内存** 。对于100w连接，单在内核内存上的占用就需要消耗： 1000000 x 8k ≈ 7.63G 的内存。

> 一个tcp连接在内核中需要占用的最小内存是：net.ipv4.tcp_wmem的最小值 + net.ipv4.tcp_rmem的最小值，根据默认配置，这个值就是 4096+4096，也就是8k。

**Socket内存上限调整**，系统内核参数中的net.ipv4.tcp_mem 是系统能够分配给tcp协议的内存限制。因此1个tcp连接占用8k内存，也就是2页，所以tcp_mem应该设置超过2\*100w=200w页。
> echo "net.ipv4.tcp_mem = 786432 2097152 3145728">> /etc/sysctl.conf
>
> 值分别是3G、6G、16G。这个值设置过小的话，如果系统资源不足以分配多余的socket，会拒绝分配socket，报“TCP:  socket out of memory”的错误。

**文件描述符上限调整**，在linux系统中，每一个socket都要占用一个文件描述符。linux系统分别从三个层面上对能打开的文件描述符设了上限。

1、系统级。linux系统规定了整个系统文件描述符上限，也就是所有进程加起来能使用的文件描述符的上限。可以通过 `cat /proc/sys/fs/file-max` 命令获取。
> 通常是由系统根据实际资源情况计算出的一个相对合理的建议值，但如果在/etc/sysctl.conf中配置过fs.file-max，则将显示配置的值。

2、shell级。表示将当前shell的当前用户所有进程能打开的最大文件数量。通过 `ulimit -n` 命令获取，默认值一般是1024。

3、用户级。ulimit -n是设置当前shell的当前用户所有进程能打开的最大文件数量，可以通过文件/etc/security/limits.conf查看，

**网络及路由器**，假设百万连接中有 20% 是活跃的, 每个连接每秒传输 1KB 的数据, 那么需要的网络带宽是 0.2M x 1KB/s x 8 = 1.6Gbps, 要求服务器至少是万兆网卡(10Gbps)。

**高并发客户端模拟**，客户端每向服务器发起一个连接就要占用一个端口，由于端口是用一个short int来表示的，因此端口最多只有65535个。端口的概念是在IP之上的，TCP或UDP才有端口的概念，IP层是没有的。也就是说理论上每个IP都可以对应65535个端口。因此可以给设置多个IP，比如多个网卡，虚拟网卡，ip别名等方式。